"""
æ ¸å¿ƒå¼•æ“ï¼šè´Ÿè´£åˆå§‹åŒ–æ¨¡å‹ã€è¿æ¥æ•°æ®åº“ã€æä¾›åŸºç¡€æ£€ç´¢åŠŸèƒ½
"""
import sys
import os
# å¯¼å…¥å·¥å…·åˆ—è¡¨ï¼Œç”¨äºç»‘å®šç»™æ¨¡å‹
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from tools import medical_tools_list

from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_chroma import Chroma

# --- 1. åŸºç¡€é…ç½® ---
DB_PATH = "./medical_db"

if not os.path.exists(DB_PATH):
    print(f"âŒ é”™è¯¯ï¼šå‘é‡åº“ä¸å­˜åœ¨ {DB_PATH}")
    print("è¯·å…ˆè¿è¡Œ python3 src/ingest_medical.py")
    sys.exit(1)

# --- 2. åˆå§‹åŒ–å…±äº«èµ„æº ---
print("âš™ï¸ æ­£åœ¨åˆå§‹åŒ–åŒ»å­¦å¼•æ“ (LLM & VectorStore)...")

# âš ï¸ å¿…é¡»ä¸å…¥åº“æ—¶ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´
embeddings = OllamaEmbeddings(model="shaw/dmeta-embedding-zh")

# åˆå§‹åŒ–ä¸»æ¨¡å‹
llm = ChatOllama(model="qwen2.5:7b", temperature=0)

# åˆå§‹åŒ–å¸¦å·¥å…·çš„æ¨¡å‹ (ç»™ Tool Agent ç”¨)
llm_with_tools = llm.bind_tools(medical_tools_list)

# è¿æ¥æ•°æ®åº“
vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

# --- 3. å°è£…é€šç”¨åŠŸèƒ½ ---

def search_knowledge_base(query: str, k: int = 3) -> str:
    """
    é€šç”¨æ£€ç´¢å‡½æ•°ï¼šè¾“å…¥é—®é¢˜ï¼Œè¿”å›æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡
    """
    print(f"  ğŸ” [å¼•æ“æ£€ç´¢] å…³é”®è¯: {query[:15]}...")
    try:
        docs = vectorstore.similarity_search(query, k=k)
        if not docs:
            return ""
        
        # æ ¼å¼åŒ–è¾“å‡º
        context = "\n\n".join([
            f"ã€æ¥æº: {d.metadata.get('title', 'æœªçŸ¥')}ã€‘\n{d.page_content}" 
            for d in docs
        ])
        return context
    except Exception as e:
        print(f"æ£€ç´¢å‡ºé”™: {e}")
        return ""