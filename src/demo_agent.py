import sys
import os
from typing import Annotated, TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

# --- 1. é…ç½®ä¸åˆå§‹åŒ– ---
DB_PATH = "./medical_db"

# æ£€æŸ¥å‘é‡åº“æ˜¯å¦å­˜åœ¨
if not os.path.exists(DB_PATH):
    print(f"âŒ é”™è¯¯ï¼šå‘é‡åº“ä¸å­˜åœ¨ {DB_PATH}")
    print("è¯·å…ˆè¿è¡Œæ•°æ®å…¥åº“è„šæœ¬ï¼")
    sys.exit(1)

class State(TypedDict):
    messages: Annotated[list, add_messages]
    context: str 

# æ¨¡å‹ï¼šå»ºè®® temperature=0 ä¿æŒä¸¥è°¨
llm = ChatOllama(model="qwen2.5:7b", temperature=0)
embeddings = OllamaEmbeddings(model="shaw/dmeta-embedding-zh")
vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)

# --- 2. èŠ‚ç‚¹å®šä¹‰ ---

def retrieve_node(state: State):
    user_query = state["messages"][-1].content
    
    # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ç”¨åŸé—®é¢˜æ£€ç´¢ï¼ˆå°æ¨¡å‹å…³é”®è¯æå–ä¸ç¨³å®šï¼‰
    search_query = user_query
    print(f"ğŸ” [ç³»ç»Ÿ] æ£€ç´¢è¯: {search_query}")
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    raw_docs = vectorstore.similarity_search(search_query, k=5)  # ç›´æ¥å–5ä¸ª
    
    # é€»è¾‘å»é‡
    unique_docs = []
    seen_titles = set()
    for doc in raw_docs:
        title = doc.metadata.get('title', '')
        if title and title not in seen_titles:
            unique_docs.append(doc)
            seen_titles.add(title)
            
    if not unique_docs:
        context_text = "æœªæ£€ç´¢åˆ°ç›¸å…³èµ„æ–™"
    else:
        context_text = "\n\n".join([
            f"ã€æ¥æº: {d.metadata.get('title', 'æœªçŸ¥')}ã€‘\n{d.page_content}" 
            for d in unique_docs
        ])
    
    print(f"ğŸ“š [ç³»ç»Ÿ] æ£€ç´¢åˆ° {len(unique_docs)} æ¡ç›¸å…³èµ„æ–™")
    return {"context": context_text}


def generate_node(state: State):
    """
    èŠ‚ç‚¹ B: æ‹Ÿç¨¿ä¸“å®¶ã€‚æ ¹æ®æ£€ç´¢ç»“æœå†™è‰ç¨¿ã€‚
    """
    query = state["messages"][-1].content
    context = state["context"]
    
    # ğŸ”§ ä¿®å¤ï¼šç®€åŒ–promptï¼Œé€‚é…å°æ¨¡å‹
    prompt = f"""ä½ æ˜¯åŒ»å­¦åŠ©æ‰‹ï¼Œæ ¹æ®ä¸‹æ–¹èµ„æ–™å›ç­”é—®é¢˜ã€‚
èµ„æ–™ä¸­æ²¡æåˆ°çš„å†…å®¹è¯´"æœªæåŠ"ï¼Œä¸è¦ç¼–é€ ã€‚

ã€èµ„æ–™ã€‘
{context}

ã€é—®é¢˜ã€‘
{query}

ã€å›ç­”ã€‘"""
    
    print("ğŸ¤– [AI] æ­£åœ¨åŸºäºèµ„æ–™ç”Ÿæˆå›ç­”è‰ç¨¿...")
    response = llm.invoke(prompt)
    return {"messages": [response]}

def human_review_node(state: State):
    """
    èŠ‚ç‚¹ C: å®¡æ ¸ç«™å°ã€‚æœ¬èº«ä¸å¹²æ´»ï¼Œåªä½œä¸ºæ–­ç‚¹ã€‚
    """
    pass

# --- 3. æ„å»ºå›¾ (å·¥ä½œæµ) ---
workflow = StateGraph(State)

workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)
workflow.add_node("human_review", human_review_node)

# è¿çº¿ï¼šå¼€å§‹ -> æ£€ç´¢ -> ç”Ÿæˆ -> å®¡æ ¸ -> ç»“æŸ
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", "human_review")
workflow.add_edge("human_review", END)

# ç¼–è¯‘ï¼šåŠ å…¥è®°å¿†å’Œæ–­ç‚¹
memory = MemorySaver()
app = workflow.compile(
    checkpointer=memory,
    interrupt_before=["human_review"] # åœ¨è¿›å…¥å®¡æ ¸å‰æš‚åœ
)

# --- 4. è¿è¡Œ Demo ---
if __name__ == "__main__":
    thread_id = "demo_session_001"
    config = {"configurable": {"thread_id": thread_id}}
    
    print("="*60)
    print("ğŸ©º ã€Šè¶…è¶Šç™¾å²ã€‹åŒ»å­¦æ™ºèƒ½åŠ©æ‰‹ (RAG + HITLç‰ˆ) å·²å¯åŠ¨")
    print("ğŸ’¡ ç‰¹æ€§ï¼šåŸºäºä¸“å±çŸ¥è¯†åº“ + åŒ»ç”Ÿå®æ—¶å®¡æ ¸æœºåˆ¶")
    print("="*60)
    
    while True:
        user_input = input("\nğŸ‘¤ æ‚£è€…æé—® (è¾“å…¥ q é€€å‡º): ")
        if user_input.lower() in ["q", "quit"]: break
        
        # 1. è¿è¡Œç›´åˆ°æ–­ç‚¹
        for event in app.stream({"messages": [HumanMessage(content=user_input)]}, config):
            pass
            
        # 2. è·å–å½“å‰çŠ¶æ€ï¼ˆAI çš„è‰ç¨¿ï¼‰
        snapshot = app.get_state(config)
        if not snapshot.values: continue
        
        ai_msg = snapshot.values["messages"][-1]
        context_used = snapshot.values.get("context", "")
        
        print("\n" + "-"*30)
        print("ğŸ‘€ [åå°] AI æ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™ç‰‡æ®µï¼š")
        print(context_used[:300] + "...") 
        print("-"*30)
        
        print(f"\nğŸ“ [å¾…å®¡æ ¸å›ç­”]:\n{ai_msg.content}")
        print("\n" + "="*30)
        
        # 3. åŒ»ç”Ÿå®¡æ ¸
        feedback = input("ğŸ‘¨â€âš•ï¸ åŒ»ç”Ÿæ“ä½œ [å›è½¦=é€šè¿‡ / è¾“å…¥æ–‡å­—=ä¿®æ”¹]: ")
        
        if feedback.strip():
            print("âœï¸  å›ç­”å·²ä¿®æ­£ã€‚")
            app.update_state(config, {"messages": [AIMessage(content=feedback)]})
        else:
            print("âœ… å®¡æ ¸é€šè¿‡ã€‚")
            
        # 4. ç»§ç»­æµç¨‹
        for event in app.stream(None, config):
            pass
            
        final_state = app.get_state(config)
        print(f"\nğŸ“¨ [å‘é€ç»™æ‚£è€…]: {final_state.values['messages'][-1].content}")