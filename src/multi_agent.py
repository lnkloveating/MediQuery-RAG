"""
Self-RAG é«˜çº§ç¼–æ’å±‚: åŒ…å«è·¯ç”±ã€åæ€æ£€ç´¢ã€é‡å†™æŸ¥è¯¢ã€è®°å¿†æŒä¹…åŒ–
"""
import sys
import os
import uuid
from typing import Annotated, TypedDict, List
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.sqlite import SqliteSaver
import sqlite3
from langchain_core.messages import AIMessage, HumanMessage

# å¯¼å…¥åŒçº§æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from tools import medical_tools_list
# ä»å¼•æ“å¯¼å…¥èµ„æº (æ³¨æ„è¿™é‡Œå¤šå¯¼å…¥äº†ä¸€ä¸ª vectorstore ç”¨äºåŸç”Ÿæ£€ç´¢)
from medical_engine import llm, llm_with_tools, search_knowledge_base, vectorstore

# --- 1. å®šä¹‰çŠ¶æ€ (æ–°å¢äº† Self-RAG éœ€è¦çš„å­—æ®µ) ---
class MultiAgentState(TypedDict):
    # æ¶ˆæ¯å†å²
    messages: Annotated[list, add_messages]
    
    # ä»»åŠ¡æ ‡å¿—
    need_tool: bool
    need_rag: bool
    need_lifestyle: bool
    
    # ç»“æœå­˜å‚¨
    tool_output: str
    rag_output: str
    lifestyle_output: str
    final_answer: str
    
    # ğŸ”¥ Self-RAG ä¸“ç”¨å­—æ®µ
    documents: List[str]   # å­˜å‚¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
    loop_step: int         # å¾ªç¯è®¡æ•°å™¨(é˜²æ­¢æ­»å¾ªç¯)

# --- 2. è¾…åŠ©å‡½æ•° (Self-RAG æ ¸å¿ƒèƒ½åŠ›) ---

def grade_documents(question: str, docs: List[str]) -> str:
    """é˜…å·è€å¸ˆï¼šåˆ¤æ–­æ–‡æ¡£æ˜¯å¦ç›¸å…³"""
    print("  âš–ï¸ [Self-RAG] æ­£åœ¨è¯„ä¼°æ–‡æ¡£è´¨é‡...")
    # ç®€å•æ‹¼æ¥å‰ä¸¤ä¸ªæ–‡æ¡£è¿›è¡Œè¯„ä¼°
    context = "\n".join(docs[:2]) 
    prompt = f"""
    ä½ æ˜¯ä¸€åè¯„åˆ†å‘˜ã€‚è¯·è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦åŒ…å«å›ç­”ç”¨æˆ·é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯ã€‚
    
    æ–‡æ¡£ç‰‡æ®µï¼š
    {context}
    
    ç”¨æˆ·é—®é¢˜ï¼š
    {question}
    
    å¦‚æœæ–‡æ¡£èƒ½éƒ¨åˆ†æˆ–å…¨éƒ¨å›ç­”é—®é¢˜ï¼Œæˆ–è€…åŒ…å«ç›¸å…³å…³é”®è¯ï¼Œå›ç­” 'yes'ã€‚
    å¦‚æœæ–‡æ¡£å®Œå…¨ä¸ç›¸å…³ï¼Œå›ç­” 'no'ã€‚
    åªå›ç­”ä¸€ä¸ªå•è¯ï¼šyes æˆ– no
    """
    score = llm.invoke(prompt).content.strip().lower()
    print(f"    ğŸ‘‰ è¯„åˆ†ç»“æœ: {score}")
    return "yes" if "yes" in score else "no"

def rewrite_query(question: str) -> str:
    """æ”¹é¢˜ä¸“å®¶ï¼šé‡å†™æœç´¢è¯"""
    print(f"  ğŸ”„ [Self-RAG] æ­£åœ¨ä¼˜åŒ–æœç´¢è¯...")
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæœç´¢å¼•æ“ä¼˜åŒ–ä¸“å®¶ã€‚åŸé—®é¢˜æ£€ç´¢æ•ˆæœä¸ä½³ï¼Œè¯·æ ¹æ®è¯­ä¹‰é‡å†™ä¸€ä¸ªæ›´å¥½çš„æœç´¢æŸ¥è¯¢è¯ã€‚
    
    åŸé—®é¢˜ï¼š{question}
    
    åªè¾“å‡ºæ–°çš„æŸ¥è¯¢è¯ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šã€‚
    """
    new_query = llm.invoke(prompt).content.strip()
    print(f"    ğŸ‘‰ æ–°æœç´¢è¯: {new_query}")
    return new_query

# --- 3. èŠ‚ç‚¹å®šä¹‰ ---

def router_node(state: MultiAgentState):
    """è·¯ç”±èŠ‚ç‚¹"""
    question = state["messages"][-1].content
    print(f"\nğŸ§­ [è·¯ç”±] åˆ†æä»»åŠ¡: {question}")
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è§„åˆ’å™¨ã€‚åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­éœ€è¦æ‰§è¡Œå“ªäº›æ­¥éª¤ã€‚
    
    é—®é¢˜ï¼š"{question}"
    
    è¯·å›ç­”ä»¥ä¸‹å…³é”®è¯ä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªï¼ˆç”¨ç©ºæ ¼éš”å¼€ï¼‰ï¼š
    - TOOL (æ¶‰åŠèº«é«˜ä½“é‡ã€è¡€å‹ã€çƒ­é‡è®¡ç®—)
    - RAG (æ¶‰åŠç–¾ç—…åŸç†ã€æ²»ç–—ã€å®šä¹‰ã€åŒ»å­¦çŸ¥è¯†)
    - LIFESTYLE (æ¶‰åŠé¥®é£Ÿã€è¿åŠ¨ã€ç¡çœ å»ºè®®)
    
    åªè¾“å‡ºå…³é”®è¯ã€‚
    """
    decision = llm.invoke(prompt).content.upper()
    print(f"  ğŸ‘‰ è§„åˆ’ç»“æœ: {decision}")
    
    return {
        "need_tool": "TOOL" in decision,
        "need_rag": "RAG" in decision,
        "need_lifestyle": "LIFESTYLE" in decision,
        "loop_step": 0, # é‡ç½®å¾ªç¯è®¡æ•°
        "documents": []
    }

def tool_node(state: MultiAgentState):
    """å·¥å…·èŠ‚ç‚¹"""
    print("ğŸ”§ [å·¥å…·Agent] æ­£åœ¨è®¡ç®—...")
    question = state["messages"][-1].content
    response = llm_with_tools.invoke(question)
    
    output = ""
    if response.tool_calls:
        results = []
        for call in response.tool_calls:
            tool = next((t for t in medical_tools_list if t.name == call["name"]), None)
            if tool:
                try:
                    res = tool.invoke(call["args"])
                    results.append(str(res))
                except Exception as e:
                    results.append(f"Error: {e}")
        output = "\n".join(results)
    
    return {"tool_output": output}

def retrieve_node(state: MultiAgentState):
    """Self-RAG: æ£€ç´¢èŠ‚ç‚¹"""
    print("ğŸ“š [Self-RAG] æ‰§è¡Œæ£€ç´¢...")
    question = state["messages"][-1].content
    
    # å¦‚æœæ˜¯é‡å†™è¿‡çš„é—®é¢˜ï¼Œå®ƒä¼šä½œä¸ºæ–°çš„ä¸€æ¡ HumanMessage å­˜åœ¨ messages é‡Œ
    # æˆ‘ä»¬å–æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºæŸ¥è¯¢è¯
    
    # ç»“åˆä¹‹å‰çš„å·¥å…·è®¡ç®—ç»“æœæ¥æœ (å¢å¼ºä¸Šä¸‹æ–‡)
    if state.get("tool_output"):
        question += f" {state['tool_output']}"

    # ä½¿ç”¨ vectorstore åŸç”Ÿæ£€ç´¢ï¼Œè·å– list
    docs = vectorstore.similarity_search(question, k=4)
    doc_contents = [d.page_content for d in docs]
    
    return {"documents": doc_contents, "loop_step": state["loop_step"] + 1}

def grade_and_generate_node(state: MultiAgentState):
    """Self-RAG: è¯„åˆ†ä¸ç”Ÿæˆå†³ç­–èŠ‚ç‚¹"""
    question = state["messages"][-1].content
    docs = state["documents"]
    
    # 1. è¯„åˆ†
    score = grade_documents(question, docs)
    
    # 2. å†³ç­–é€»è¾‘
    if score == "yes" or state["loop_step"] >= 3:
        # å¦‚æœç›¸å…³ï¼Œæˆ–è€…å·²ç»é‡è¯•äº†3æ¬¡ï¼Œå°±å¼ºåˆ¶ç”Ÿæˆ
        if score == "no":
            print("  âš ï¸ é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œå¼ºåˆ¶ç”Ÿæˆå›ç­”ã€‚")
        
        print("ğŸ’¡ [Self-RAG] ç”Ÿæˆæœ€ç»ˆå›ç­”...")
        context = "\n\n".join(docs)
        prompt = f"åŸºäºèµ„æ–™å›ç­”ï¼š\nèµ„æ–™ï¼š{context}\né—®é¢˜ï¼š{question}"
        answer = llm.invoke(prompt).content
        return {"rag_output": answer, "final_answer": "ready"} # æ ‡è®°å®Œæˆ
        
    else:
        # 3. å¦‚æœä¸ç›¸å…³ä¸”æ²¡è¶…é™ -> é‡å†™é—®é¢˜
        new_query = rewrite_query(question)
        # å°†æ–°é—®é¢˜åŠ å…¥å†å²ï¼Œä¾›ä¸‹ä¸€è½®æ£€ç´¢ä½¿ç”¨
        return {"messages": [HumanMessage(content=new_query)]}

def lifestyle_node(state: MultiAgentState):
    """ç”Ÿæ´»å»ºè®®èŠ‚ç‚¹"""
    print("ğŸƒ [ç”Ÿæ´»æ–¹å¼Agent] ç”Ÿæˆå»ºè®®...")
    question = state["messages"][-1].content
    query = f"è¿åŠ¨ é¥®é£Ÿ ç¡çœ  é¢„é˜² {question}"
    context = search_knowledge_base(query, k=4)
    
    prompt = f"åŸºäºä»¥ä¸‹èµ„æ–™æä¾›ç”Ÿæ´»å»ºè®®ï¼š\nèµ„æ–™ï¼š{context}\né—®é¢˜ï¼š{question}"
    advice = llm.invoke(prompt).content
    return {"lifestyle_output": advice}

def summarizer_node(state: MultiAgentState):
    """æ€»ç»“èŠ‚ç‚¹"""
    print("ğŸ“Š [æ€»ç»“Agent] æ•´åˆè¾“å‡º...")
    parts = []
    if state.get("tool_output"): parts.append(f"ğŸ“‹ ã€å¥åº·è¯„ä¼°ã€‘\n{state['tool_output']}")
    if state.get("rag_output"): parts.append(f"ğŸ“– ã€åŒ»å­¦çŸ¥è¯†ã€‘\n{state['rag_output']}")
    if state.get("lifestyle_output"): parts.append(f"ğŸ’¡ ã€ç”Ÿæ´»å»ºè®®ã€‘\n{state['lifestyle_output']}")
    
    final_text = "\n\n" + "="*30 + "\n\n".join(parts) if parts else "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”ã€‚"
    return {"final_answer": final_text, "messages": [AIMessage(content=final_text)]}

# --- 4. æ„å»ºå›¾é€»è¾‘ ---
workflow = StateGraph(MultiAgentState)

workflow.add_node("router", router_node)
workflow.add_node("tool_agent", tool_node)
workflow.add_node("lifestyle_agent", lifestyle_node)
workflow.add_node("summarizer", summarizer_node)

# Self-RAG å­å›¾èŠ‚ç‚¹
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("grade_loop", grade_and_generate_node)

# èµ·ç‚¹
workflow.add_edge(START, "router")

# Router è·¯ç”±é€»è¾‘
def route_after_router(state):
    # å¹¶è¡Œ/ä¸²è¡Œé€»è¾‘ï¼šä¼˜å…ˆå¤„ç† Toolï¼Œç„¶å RAGï¼Œæœ€å Lifestyle
    if state["need_tool"]: return "tool_agent"
    if state["need_rag"]: return "retrieve"
    if state["need_lifestyle"]: return "lifestyle_agent"
    return "summarizer"

workflow.add_conditional_edges("router", route_after_router)

# Tool åçš„è·¯ç”±
def route_after_tool(state):
    if state["need_rag"]: return "retrieve"
    if state["need_lifestyle"]: return "lifestyle_agent"
    return "summarizer"

workflow.add_conditional_edges("tool_agent", route_after_tool)

# Self-RAG å†…éƒ¨å¾ªç¯é€»è¾‘
workflow.add_edge("retrieve", "grade_loop")

def route_self_rag(state):
    # æ£€æŸ¥ grade_and_generate_node çš„è¾“å‡º
    # å¦‚æœç”Ÿæˆäº† rag_output (å³ final_answer == 'ready')ï¼Œåˆ™é€€å‡ºå¾ªç¯
    if state.get("final_answer") == "ready":
        # RAG ç»“æŸåï¼Œçœ‹æ˜¯å¦éœ€è¦ç”Ÿæ´»å»ºè®®
        if state["need_lifestyle"]: return "lifestyle_agent"
        return "summarizer"
    else:
        # å¦åˆ™å›ç‚‰é‡é€ ï¼ˆåˆ©ç”¨é‡å†™åçš„ query å†æ¬¡æ£€ç´¢ï¼‰
        return "retrieve"

workflow.add_conditional_edges("grade_loop", route_self_rag, 
    {"lifestyle_agent": "lifestyle_agent", "summarizer": "summarizer", "retrieve": "retrieve"}
)

# Lifestyle åçš„è·¯ç”±
workflow.add_edge("lifestyle_agent", "summarizer")
workflow.add_edge("summarizer", END)

# --- 5. è¿è¡Œ (å¸¦æŒä¹…åŒ–) ---
conn = sqlite3.connect("chat_history.db", check_same_thread=False)
memory = SqliteSaver(conn)
app = workflow.compile(checkpointer=memory)

if __name__ == "__main__":
    print("="*50)
    print("ğŸš€ Self-RAG åŒ»å­¦ä¸“å®¶ç³»ç»Ÿ (å«åæ€èƒ½åŠ›)")
    print("ğŸ‘‰ /new: æ–°å¯¹è¯ | /clear: æ¸…ç©ºè®°å¿† | q: é€€å‡º")
    print("="*50)
    
    # åˆå§‹åŒ– ID
    user_id = input("è¯·è¾“å…¥ä¼šè¯ID (å›è½¦æ–°ID): ").strip()
    thread_id = user_id if user_id else str(uuid.uuid4())
    print(f"âœ¨ å½“å‰ID: {thread_id}\n" + "-"*30)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚£è€…: ").strip()
            if user_input.lower() in ["q", "quit"]: break
            
            # è®°å¿†ç®¡ç†æŒ‡ä»¤
            if user_input == "/new":
                thread_id = str(uuid.uuid4())
                print(f"ğŸ§¹ æ–°ä¼šè¯: {thread_id}"); continue
            
            if user_input == "/clear":
                conn.cursor().execute("DELETE FROM checkpoints WHERE thread_id = ?", (thread_id,))
                conn.commit()
                print("âœ… è®°å¿†å·²æ¸…é™¤"); continue
            
            if not user_input: continue

            config = {"configurable": {"thread_id": thread_id}}
            
            # è¿è¡Œå¹¶æ‰“å°æœ€ç»ˆç»“æœ
            # æ³¨æ„ï¼šSelf-RAG ä¸­é—´æ­¥éª¤å¤šï¼Œstream_mode="values" ä¼šæ‰“å°å¾ˆå¤šè¿‡ç¨‹
            # è¿™é‡Œæˆ‘ä»¬åªæ‰“å°æœ€ç»ˆç»“æœï¼Œä¸­é—´è¿‡ç¨‹é€šè¿‡ print è°ƒè¯•ä¿¡æ¯æŸ¥çœ‹
            final_res = None
            for event in app.stream({"messages": [HumanMessage(content=user_input)]}, config):
                # å®æ—¶æ•è·æœ€ç»ˆç»“æœ
                if "summarizer" in event:
                    final_res = event["summarizer"]["final_answer"]
            
            if final_res:
                print(final_res)
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")